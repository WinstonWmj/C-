{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Pytorch实现注意力机制、多头注意力与自注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 128]) torch.Size([1, 4, 128]) torch.Size([1, 128, 4])\n",
      "u.shape torch.Size([1, 2, 4])\n",
      "u.shape after mask torch.Size([1, 2, 4])\n",
      "attn.shape torch.Size([1, 2, 4])\n",
      "output.shape torch.Size([1, 2, 64])\n",
      "torch.Size([1, 2, 4])\n",
      "torch.Size([1, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "# 01. Pytorch实现注意力机制、多头注意力与自注意力\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\" Scaled Dot-Product Attention \"\"\"\n",
    "\n",
    "    def __init__(self, scale):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "        self.softmax = nn.Softmax(dim=2)  # 仅对最后一个维度进行softmax，这确保了每个查询（q）位置对所有键（k）的注意力权重之和为1\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        print(q.shape, k.shape, k.transpose(1, 2).shape)\n",
    "        u = torch.bmm(q, k.transpose(1, 2)) # 1.Matmul 矩阵乘法：计算查询矩阵(q)和键矩阵(k)的点积，使用torch.bmm进行批量矩阵乘法\n",
    "        print('u.shape', u.shape)\n",
    "        u = u / self.scale # 2.Scale 缩放：将点积结果除以缩放因子（通常是键向量维度的平方根， 因为假设了方差为1， 均值为0， 所以除以根号d_k）\n",
    "\n",
    "        if mask is not None:\n",
    "            u = u.masked_fill(mask, -np.inf) # 3.Mask 掩码，将false的位置填充为-inf， 这样在softmax时，这些位置的值会趋近于0，实现了注意力的选择性关注；\n",
    "        print('u.shape after mask', u.shape)\n",
    "\n",
    "        attn = self.softmax(u) # 4.Softmax 归一化，将u中的值归一化到0-1之间， 这样每个位置的值表示该位置与其他位置的关联度；\n",
    "        print('attn.shape', attn.shape)\n",
    "        output = torch.bmm(attn, v) # 5.Output 加权求和，将归一化后的注意力权重与值矩阵相乘，得到加权求和的结果；\n",
    "        print('output.shape', output.shape)\n",
    "\n",
    "        return attn, output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_q, n_k, n_v = 2, 4, 4\n",
    "    d_q, d_k, d_v = 128, 128, 64\n",
    "    batch = 1\n",
    "    q = torch.randn(batch, n_q, d_q)\n",
    "    k = torch.randn(batch, n_k, d_k)\n",
    "    v = torch.randn(batch, n_v, d_v)\n",
    "    mask = torch.zeros(batch, n_q, n_k).bool()\n",
    "\n",
    "    attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
    "    attn, output = attention(q, k, v, mask=mask)\n",
    "\n",
    "    print(attn.shape)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多头注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"多头自注意力机制\n",
    "    特点：\n",
    "        1. 每个头有自己独特的参数（投影矩阵的一部分）\n",
    "        2. 每个头关注输入的不同方面\n",
    "        3. 最后的输出投影层将所有头的输出组合在一起\n",
    "    \n",
    "    参数:\n",
    "        dim_in (int): 输入维度\n",
    "        dim_k (int): 键和查询的维度\n",
    "        dim_v (int): 值的维度\n",
    "        num_heads (int): 注意力头的数量\n",
    "    \n",
    "    注意:\n",
    "        - dim_k 和 dim_v 必须能被 num_heads 整除\n",
    "        - 每个头的维度为: dk = dim_k // num_heads, dv = dim_v // num_heads\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim_in, dim_k, dim_v, num_heads=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 参数检查\n",
    "        if dim_k % num_heads != 0 or dim_v % num_heads != 0:\n",
    "            raise ValueError(\"dim_k和dim_v必须是num_heads的整数倍\")\n",
    "            \n",
    "        # 保存配置参数\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_k = dim_k \n",
    "        self.dim_v = dim_v\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim_k = dim_k // num_heads  # 每个头的键/查询维度\n",
    "        self.head_dim_v = dim_v // num_heads  # 每个头的值维度\n",
    "        \n",
    "        # 线性变换层\n",
    "        self.q_proj = nn.Linear(dim_in, dim_k, bias=False)\n",
    "        self.k_proj = nn.Linear(dim_in, dim_k, bias=False)\n",
    "        self.v_proj = nn.Linear(dim_in, dim_v, bias=False)\n",
    "        self.output_proj = nn.Linear(dim_v, dim_in)  # 新增：输出投影\n",
    "        \n",
    "        # 缩放因子\n",
    "        self.scale = 1 / math.sqrt(self.head_dim_k)  # 从math模块导入sqrt\n",
    "        \n",
    "        # 初始化参数\n",
    "        self._reset_parameters()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        \"\"\"初始化模型参数\"\"\"\n",
    "        # 使用xavier初始化提高训练稳定性\n",
    "        \"\"\"Xavier初始化（也叫Glorot初始化）的目的是：\n",
    "            保持每一层输入和输出的方差大致相等\n",
    "            防止深层网络中的梯度消失或爆炸\n",
    "            计算方法：权重从均值为0，方差为2/(fan_in + fan_out)的均匀分布中采样\n",
    "            fan_in：输入特征数\n",
    "            fan_out：输出特征数\"\"\"\n",
    "        nn.init.xavier_uniform_(self.q_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.k_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.v_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.output_proj.weight)\n",
    "    \n",
    "    def _reshape_to_batches(self, x, batch_size, seq_len, dim):\n",
    "        \"\"\"将输入重塑为多头格式\"\"\"\n",
    "        # x: (batch, seq_len, dim)\n",
    "        x = x.reshape(batch_size, seq_len, self.num_heads, dim // self.num_heads)\n",
    "        # 原始: (batch, seq_len, 512)\n",
    "        # reshape后: (batch, seq_len, 8, 64)\n",
    "        # 每个头的梯度会传回到对应的投影矩阵部分\n",
    "        # 不同头的参数会得到不同的梯度更新\n",
    "\n",
    "        # 转置以让head维度在前\n",
    "        x = x.transpose(1, 2)  # (batch, num_heads, seq_len, head_dim)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"前向传播\n",
    "        \n",
    "        参数:\n",
    "            x: 输入张量, shape (batch_size, seq_len, dim_in)\n",
    "            mask: 可选的注意力掩码, shape (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        返回:\n",
    "            output: 输出张量, shape (batch_size, seq_len, dim_in)\n",
    "            attention: 注意力权重, shape (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, dim_in = x.shape\n",
    "        assert dim_in == self.dim_in, f\"输入维度{dim_in}与期望维度{self.dim_in}不匹配\"\n",
    "        \n",
    "        # 1. 线性投影\n",
    "        q = self.q_proj(x)  # (batch, seq_len, dim_k)\n",
    "        k = self.k_proj(x)  # (batch, seq_len, dim_k)\n",
    "        v = self.v_proj(x)  # (batch, seq_len, dim_v)\n",
    "        \n",
    "        # 2. 重塑为多头形式\n",
    "        q = self._reshape_to_batches(q, batch_size, seq_len, self.dim_k)  # (batch, num_heads, seq_len, seq_len)\n",
    "        k = self._reshape_to_batches(k, batch_size, seq_len, self.dim_k)  # (batch, num_heads, seq_len, seq_len)\n",
    "        v = self._reshape_to_batches(v, batch_size, seq_len, self.dim_v)  # (batch, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # 3. 计算注意力分数\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2, -1))  # (batch, num_heads, seq_len, seq_len)\n",
    "        attention_scores = attention_scores * self.scale\n",
    "        \n",
    "        # 4. 应用掩码（如果提供）\n",
    "        if mask is not None:\n",
    "            # 扩展mask以适应多头\n",
    "            mask = mask.unsqueeze(1)  # (batch, 1, seq_len, seq_len)\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # 5. 注意力权重\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # 6. 计算输出\n",
    "        output = torch.matmul(attention_weights, v)  # (batch, num_heads, seq_len, head_dim_v)\n",
    "        \n",
    "        # 7. 重组多头输出\n",
    "        output = output.transpose(1, 2)  # (batch, seq_len, num_heads, head_dim_v)\n",
    "        output = output.reshape(batch_size, seq_len, self.dim_v)  # (batch, seq_len, dim_v)\n",
    "        \n",
    "        # 8. 最终输出投影\n",
    "        output = self.output_proj(output)  # (batch, seq_len, dim_in)\n",
    "        \n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output torch.Size([32, 50, 512])\n",
      "attention torch.Size([32, 8, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "# 使用示例\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "dim_in = 512\n",
    "dim_k = 512\n",
    "dim_v = 512\n",
    "num_heads = 8\n",
    "\n",
    "# 创建模型\n",
    "mha = MultiHeadSelfAttention(\n",
    "    dim_in=dim_in,\n",
    "    dim_k=dim_k,\n",
    "    dim_v=dim_v,\n",
    "    num_heads=num_heads\n",
    ")\n",
    "\n",
    "# 创建输入\n",
    "x = torch.randn(batch_size, seq_len, dim_in)\n",
    "mask = torch.ones(batch_size, seq_len, seq_len)  # 可选的掩码\n",
    "\n",
    "# 前向传播\n",
    "output, attention = mha(x, mask)\n",
    "print('output', output.shape)\n",
    "print('attention', attention.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量创建\n",
    "\n",
    "\n",
    "1. **数据类型**：\n",
    "- torch.tensor会自动推断类型\n",
    "- torch.Tensor默认是float32\n",
    "- 明确指定类型可以避免意外\n",
    "\n",
    "2. **内存共享**：\n",
    "- from_numpy和as_tensor尽可能共享内存\n",
    "- tensor总是创建新的内存\n",
    "\n",
    "3. **设备放置**：\n",
    "- 创建时指定device更高效\n",
    "- 后续移动到GPU会产生复制\n",
    "\n",
    "4. **梯度计算**：\n",
    "- 只有浮点类型张量支持梯度计算\n",
    "- requires_grad=True只对浮点张量有效\n",
    "\n",
    "这些方法各有特点，选择合适的创建方式可以提高代码的效率和可维护性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. torch.tensor vs torch.Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 cpu False\n",
      "torch.float32 cpu True\n",
      "torch.float32 cpu False\n"
     ]
    }
   ],
   "source": [
    "### torch.tensor()\n",
    "\n",
    "x = torch.tensor([1, 2, 3])  # 推荐使用这种方式\n",
    "\n",
    "# 特点：\n",
    "# - 会根据输入数据推断数据类型\n",
    "# - 创建新的张量，不共享内存\n",
    "# - 更安全，更现代的API\n",
    "# - 可以指定设备和数据类型\n",
    "\n",
    "print(x.dtype, x.device, x.requires_grad)\n",
    "x = torch.tensor([1, 2, 3], \n",
    "                dtype=torch.float32,\n",
    "                # device='cuda:0',\n",
    "                requires_grad=True)\n",
    "\n",
    "print(x.dtype, x.device, x.requires_grad)\n",
    "### torch.Tensor\n",
    "\n",
    "x = torch.Tensor([1, 2, 3])  # 不推荐\n",
    "\n",
    "# 特点：\n",
    "# - 默认数据类型是float32\n",
    "# - 是torch.FloatTensor的别名\n",
    "# - 旧式API，不够灵活\n",
    "print(x.dtype, x.device, x.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. 特定数值的张量创建\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 cpu False\n",
      "torch.float32 cpu False\n",
      "torch.float32 cpu False\n",
      "torch.float32 cpu False\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.float32 cpu False\n",
      "torch.float32 cpu False\n",
      "torch.int64 cpu False\n",
      "torch.int64 cpu False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### 全0张量\n",
    "\n",
    "# 方法1：zeros\n",
    "x = torch.zeros(2, 3)\n",
    "# 方法2：zeros_like\n",
    "y = torch.zeros_like(x)  # 创建与x形状相同的全0张量\n",
    "print(x.dtype, x.device, x.requires_grad)\n",
    "print(y.dtype, y.device, y.requires_grad)\n",
    "### 全1张量\n",
    "\n",
    "# 方法1：ones\n",
    "x = torch.ones(2, 3)\n",
    "# 方法2：ones_like\n",
    "y = torch.ones_like(x)\n",
    "print(x.dtype, x.device, x.requires_grad)\n",
    "print(y.dtype, y.device, y.requires_grad)\n",
    "\n",
    "# empty: 不初始化\n",
    "x = torch.empty(2, 3)  # 未初始化的张量\n",
    "print(x)\n",
    "\n",
    "### 指定值填充\n",
    "\n",
    "x = torch.full((2, 3), 7.)  # 创建形状为(2,3)的张量，填充值为7\n",
    "y = torch.full_like(x, 3)  # 创建与x形状相同的张量，填充值为3\n",
    "print(x.dtype, x.device, x.requires_grad)\n",
    "print(y.dtype, y.device, y.requires_grad)\n",
    "\n",
    "x = torch.full((2, 3), 7)  # 创建形状为(2,3)的张量，填充值为7\n",
    "y = torch.full_like(x, 3)  # 创建与x形状相同的张量，填充值为3\n",
    "print(x.dtype, x.device, x.requires_grad)\n",
    "print(y.dtype, y.device, y.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. 随机张量，符合均匀分布、正态分布，伯努利，指数分布\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2566, 0.7936, 0.9408],\n",
      "        [0.1332, 0.9346, 0.5936]])\n",
      "二值分布 tensor([[0., 1., 1.],\n",
      "        [1., 1., 0.]])\n",
      "Dropout掩码 tensor([[0., 0., 0.],\n",
      "        [2., 0., 2.]])\n",
      "指数分布 tensor([[0.2902, 0.2658, 0.2558],\n",
      "        [0.3821, 0.9530, 0.9107]])\n",
      "泊松分布 tensor([[2., 4., 5.],\n",
      "        [4., 4., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1204713f0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 均匀分布 光秃秃是均匀 uniform\n",
    "# [0,1)均匀分布\n",
    "x = torch.rand(2, 3)\n",
    "\n",
    "# [a,b)均匀分布\n",
    "x = torch.empty(2, 3)  # 创建一个2x3的张量，但不初始化值\n",
    "x.uniform_(0, 1)  # 用均匀分布初始化x\n",
    "print(x)\n",
    "\n",
    "\n",
    "### 正态分布 n是normal\n",
    "\n",
    "# 标准正态分布 N(0,1)\n",
    "x = torch.randn(2, 3)\n",
    "\n",
    "# 指定均值和标准差的正态分布\n",
    "mean = 0.0\n",
    "std = 1.0\n",
    "x = torch.normal(mean, std, size=(2, 3))\n",
    "x = torch.distributions.Normal(mean, std).sample((2, 3))\n",
    "\n",
    "# 生成0-1二值分布\n",
    "prob = 0.5\n",
    "x = torch.bernoulli(torch.full((2, 3), prob))\n",
    "print(\"二值分布\", x)\n",
    "# Dropout掩码生成\n",
    "p = 0.5\n",
    "mask = torch.bernoulli(torch.full((2, 3), 1 - p)) / (1 - p)\n",
    "print(\"Dropout掩码\", mask)\n",
    "# 指数分布\n",
    "rate = 1.0\n",
    "x = torch.distributions.Exponential(rate).sample((2, 3))\n",
    "print(\"指数分布\", x)\n",
    "# 泊松分布\n",
    "rate = 4.0\n",
    "x = torch.poisson(torch.full((2, 3), rate))\n",
    "print(\"泊松分布\", x)\n",
    "# 设置随机种子以保证结果可复现\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 序列张量创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "等差序列 tensor([0, 2, 4, 6, 8])\n",
      "等差序列 tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])\n",
      "等比序列 tensor([  1.0000,   3.1623,  10.0000,  31.6228, 100.0000])\n"
     ]
    }
   ],
   "source": [
    "### 等差序列\n",
    "\n",
    "# 方法1：arange\n",
    "x = torch.arange(start=0, end=10, step=2)  # [0,2,4,6,8]\n",
    "print(\"等差序列\", x)\n",
    "# 方法2：linspace\n",
    "x = torch.linspace(start=0, end=10, steps=5)  # 均匀分成5份\n",
    "print(\"等差序列\", x)\n",
    "\n",
    "### 等比序列\n",
    "\n",
    "x = torch.logspace(start=0, end=2, steps=5)  # 10^0到10^2均匀分成5份\n",
    "print(\"等比序列\", x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. 从其他格式转换\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 从NumPy数组\n",
    "\n",
    "import numpy as np\n",
    "arr = np.array([1, 2, 3])\n",
    "x = torch.from_numpy(arr)  # 共享内存\n",
    "y = torch.tensor(arr)      # 复制数据\n",
    "\n",
    "\n",
    "### 从列表\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.as_tensor([1, 2, 3])  # 尽可能避免复制\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 特殊矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对角矩阵\n",
      " tensor([[1, 0, 0],\n",
      "        [0, 2, 0],\n",
      "        [0, 0, 3]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### 单位矩阵\n",
    "\n",
    "x = torch.eye(3)  # 3x3单位矩阵\n",
    "\n",
    "\n",
    "### 对角矩阵\n",
    "\n",
    "x = torch.diag(torch.tensor([1, 2, 3]))\n",
    "print(\"对角矩阵\\n\", x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7. 高级创建方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8009+0.1713j, 0.3037+0.8155j],\n",
       "        [0.1234+0.3876j, 0.1526+0.2347j]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### 稀疏张量\n",
    "\n",
    "i = torch.tensor([[0, 1, 1],\n",
    "                 [2, 0, 2]])\n",
    "v = torch.tensor([3, 4, 5])\n",
    "x = torch.sparse_coo_tensor(i, v, (3, 3))\n",
    "\n",
    "\n",
    "### 复数张量\n",
    "\n",
    "x = torch.complex(torch.rand(2, 2), torch.rand(2, 2))\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8. 实际应用建议\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. **日常使用**：\n",
    "\n",
    "# 推荐使用torch.tensor\n",
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "\n",
    "# 2. **从NumPy转换**：\n",
    "\n",
    "# 如果不需要共享内存\n",
    "numpy_array = np.array([1, 2, 3])\n",
    "x = torch.tensor(numpy_array)\n",
    "# 如果需要共享内存\n",
    "x = torch.from_numpy(numpy_array)\n",
    "\n",
    "\n",
    "# 3. **性能优化**：\n",
    "\n",
    "# 避免频繁创建新张量\n",
    "new_data = torch.randn(1000, 1000)\n",
    "# x = torch.zeros(1000, 1000)  # 预分配\n",
    "x = torch.empty(1000, 1000)  # 预分配\n",
    "x.copy_(new_data)  # 原地更新\n",
    "\n",
    "\n",
    "# 4. **GPU使用**：\n",
    "\n",
    "# 直接在GPU上创建\n",
    "x = torch.tensor([1, 2, 3], device='cpu')\n",
    "# 或者后续移动到GPU\n",
    "# x = x.cuda()  # 或 x = x.to('cuda')\n",
    "\n",
    "\n",
    "# 5. **梯度计算**：\n",
    "\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 填充（padding）和掩码（masking）相关操作：\n",
    "\n",
    "1. **内存效率**：\n",
    "- pack_padded_sequence可以减少计算量\n",
    "- masked_fill比循环填充更高效\n",
    "\n",
    "2. **计算速度**：\n",
    "- 批量操作通常比循环处理更快\n",
    "- 适当的填充可以提高并行计算效率\n",
    "\n",
    "3. **建议**：\n",
    "- 对于RNN，优先使用pack_padded_sequence\n",
    "- 图像处理建议使用F.pad\n",
    "- 注意力机制中使用masked_fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. 序列填充操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里还不是很懂。\n",
    "### 1.1 pad_sequence\n",
    "\n",
    "# 处理变长序列\n",
    "sequences = [torch.randn(3, 4), torch.randn(5, 4), torch.randn(2, 4)]\n",
    "padded = torch.nn.utils.rnn.pad_sequence(sequences, \n",
    "                                       batch_first=True,\n",
    "                                       padding_value=0.0)\n",
    "\n",
    "# 特点：\n",
    "# - 自动将短序列填充到最长序列长度\n",
    "# - 可以选择batch_first参数调整输出格式\n",
    "# - 默认用0填充，可通过padding_value指定填充值\n",
    "# - 适用于RNN/LSTM的批处理\n",
    "\n",
    "### 1.2 pack_padded_sequence\n",
    "\n",
    "# 压缩填充序列\n",
    "packed = torch.nn.utils.rnn.pack_padded_sequence(padded_sequence,\n",
    "                                                lengths,\n",
    "                                                batch_first=True,\n",
    "                                                enforce_sorted=False)\n",
    "\n",
    "# 特点：\n",
    "# - 将填充后的序列压缩，提高计算效率\n",
    "# - 需要提供原始序列长度信息\n",
    "# - 通常与RNN层配合使用\n",
    "# - enforce_sorted控制是否需要按长度排序\n",
    "\n",
    "### 1.3 pad_packed_sequence\n",
    "\n",
    "# 解压压缩序列\n",
    "unpacked, lengths = torch.nn.utils.rnn.pad_packed_sequence(packed_sequence,\n",
    "                                                         batch_first=True,\n",
    "                                                         padding_value=0.0)\n",
    "\n",
    "# 特点：\n",
    "# - 将压缩序列还原为填充形式\n",
    "# - 返回填充后的序列和长度信息\n",
    "# - 常用于RNN输出处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 张量填充操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0499,  0.5263, -0.0085,  0.7291,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.1331,  0.8640, -1.0157, -0.8887,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.1498, -0.2089, -0.3870,  0.9912,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### 2.1 F.pad\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 张量边缘填充\n",
    "input = torch.randn(3, 4)\n",
    "padded = F.pad(input, pad=(1, 2, 3, 4), mode='constant', value=0)  # 左右上下\n",
    "print(padded)\n",
    "# 特点：\n",
    "# - 支持多种填充模式：\n",
    "#   - constant: 常数填充\n",
    "#   - reflect: 反射填充\n",
    "#   - replicate: 复制边缘值\n",
    "#   - circular: 循环填充\n",
    "# - 常用于图像处理和卷积层\n",
    "\n",
    "### 2.2 常用填充模式示例\n",
    "\n",
    "x = torch.randn(3, 4) \n",
    "# 常数填充\n",
    "padded_constant = F.pad(x, (1, 1), mode='constant', value=0)\n",
    "\n",
    "# 反射填充\n",
    "padded_reflect = F.pad(x, (1, 1), mode='reflect')\n",
    "\n",
    "# 复制填充\n",
    "padded_replicate = F.pad(x, (1, 1), mode='replicate')\n",
    "\n",
    "# 循环填充\n",
    "padded_circular = F.pad(x, (1, 1), mode='circular')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. 掩码操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 3.1 masked_fill\n",
    "\n",
    "# 条件填充\n",
    "mask = torch.zeros(3, 4).bool()\n",
    "tensor = torch.randn(3, 4)\n",
    "masked = tensor.masked_fill(mask, value=-float('inf'))\n",
    "\n",
    "# 特点：\n",
    "# - 根据掩码选择性填充\n",
    "# - 常用于注意力机制\n",
    "# - 可以填充任意值\n",
    "\n",
    "### 3.2 masked_select\n",
    "\n",
    "# 条件选择\n",
    "mask = tensor > 0\n",
    "selected = torch.masked_select(tensor, mask)\n",
    "\n",
    "# 特点：\n",
    "# - 提取满足条件的元素\n",
    "# - 返回一维张量\n",
    "# - 用于数据过滤\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. 实际应用场景\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4625,  0.6699,  0.8244, -0.5473],\n",
      "        [-0.0600, -0.6364,  0.6168,  0.5412],\n",
      "        [-0.4060,  0.6554,  0.1888,  0.2390]]) torch.Size([3, 4])\n",
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]]) torch.Size([3, 4])\n",
      "tensor([[[False, False, False, False]],\n",
      "\n",
      "        [[False, False, False, False]],\n",
      "\n",
      "        [[False, False, False, False]]]) torch.Size([3, 1, 4])\n",
      "tensor([[[-1.4625,  0.6699,  0.8244, -0.5473],\n",
      "         [-0.0600, -0.6364,  0.6168,  0.5412],\n",
      "         [-0.4060,  0.6554,  0.1888,  0.2390]],\n",
      "\n",
      "        [[-1.4625,  0.6699,  0.8244, -0.5473],\n",
      "         [-0.0600, -0.6364,  0.6168,  0.5412],\n",
      "         [-0.4060,  0.6554,  0.1888,  0.2390]],\n",
      "\n",
      "        [[-1.4625,  0.6699,  0.8244, -0.5473],\n",
      "         [-0.0600, -0.6364,  0.6168,  0.5412],\n",
      "         [-0.4060,  0.6554,  0.1888,  0.2390]]]) torch.Size([3, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### 4.1 自然语言处理\n",
    "\n",
    "# 批量处理变长文本\n",
    "texts = [\"你好\", \"你好世界\", \"你好pytorch\"]\n",
    "# 转换为索引序列\n",
    "sequences = [[1, 2], [1, 2, 3, 4], [1, 2, 5, 6, 7]]\n",
    "# 填充处理\n",
    "padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(s) for s in sequences], \n",
    "                                       batch_first=True)\n",
    "\n",
    "\n",
    "### 4.2 计算机视觉\n",
    "\n",
    "# 图像批处理填充\n",
    "images = [torch.randn(3, 224, 224), torch.randn(3, 256, 256)]\n",
    "# 填充到相同大小\n",
    "max_w, max_h = 0, 0\n",
    "padded_images = [F.pad(img, \n",
    "                      (0, max_w - img.size(-1), 0, max_h - img.size(-2))) \n",
    "                 for img in images]\n",
    "\n",
    "\n",
    "### 4.3 注意力机制\n",
    "\n",
    "# 注意力掩码\n",
    "scores = torch.randn(3, 4)\n",
    "batch_size = 3\n",
    "seq_len = 4\n",
    "attention_mask = torch.ones(batch_size, seq_len).bool()\n",
    "print(scores, scores.shape)\n",
    "print(attention_mask, attention_mask.shape)\n",
    "\n",
    "attention_mask = ~attention_mask.unsqueeze(1)\n",
    "print(attention_mask, attention_mask.shape)\n",
    "\n",
    "scores = scores.masked_fill(attention_mask, -float('inf'))\n",
    "print(scores, scores.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. *符号或torch.mul 逐元素乘法运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4, 10, 18])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "c = a * b  # 结果: [4, 10, 18] \n",
    "print(c)\n",
    "# 用途：逐元素乘法\n",
    "# 特点：\n",
    "# 逐元素操作\n",
    "# 支持广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1207, -0.4596, -1.7579,  0.6972],\n",
      "         [ 0.4776,  0.5133,  5.5293,  0.1820],\n",
      "         [ 0.6311,  0.2602,  0.2450, -0.2941]],\n",
      "\n",
      "        [[-0.0852,  0.0782,  0.6269,  0.9053],\n",
      "         [-0.4101, -0.2976,  4.9137, -0.5574],\n",
      "         [ 0.5847, -0.3367,  2.1574,  1.6245]]]) torch.Size([2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0555, -0.4722, -0.4096, -1.6197],\n",
       "         [ 0.4276,  0.4707, -0.0342, -3.0549],\n",
       "         [-0.0234,  1.5303,  1.2431,  0.0591]],\n",
       "\n",
       "        [[ 1.0555, -0.4722, -0.4096, -1.6197],\n",
       "         [ 0.4276,  0.4707, -0.0342, -3.0549],\n",
       "         [-0.0234,  1.5303,  1.2431,  0.0591]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 支持广播机制\n",
    "A = torch.randn(2, 3, 4)\n",
    "b = torch.randn(4)  # 将被广播到 [2, 3, 4]\n",
    "C = A * b  # 可以运算\n",
    "print(C, C.shape)\n",
    "# 但要注意维度匹配\n",
    "A1 = torch.randn(3, 4)\n",
    "# 方法1: stack沿着新维度堆叠\n",
    "A = torch.stack([A1, A1], dim=0)  # shape: [2, 3, 4]\n",
    "# 或者用cat/vstack等其他方法:\n",
    "# A = torch.cat([A1.unsqueeze(0), A1.unsqueeze(0)], dim=0)\n",
    "B = torch.randn(1, 3, 4)  # 中间维度不匹配且不能广播\n",
    "C = A * B  # 这会报错！\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 广播机制的基本原则\n",
    "广播（Broadcasting）是PyTorch自动处理不同形状张量运算的一种机制，遵循以下规则：\n",
    "\n",
    "从右向左对齐维度\n",
    "\n",
    "维度为1的可以广播到任意大小\n",
    "\n",
    "缺失的维度会自动补充为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 6])\n",
      "tensor([[5, 6, 7],\n",
      "        [6, 7, 8],\n",
      "        [7, 8, 9]])\n",
      "tensor([[[-2.7964, -1.6647, -0.8599,  3.3309],\n",
      "         [ 1.4293,  1.3316,  0.4652,  2.9003],\n",
      "         [ 1.0565, -0.0409,  2.0614,  3.5634]],\n",
      "\n",
      "        [[-2.6389, -2.0402,  1.6642,  0.9286],\n",
      "         [ 0.4553, -0.0277,  1.8422,  1.9993],\n",
      "         [-0.6844, -0.1489,  1.6114,  2.5484]]])\n"
     ]
    }
   ],
   "source": [
    "# 示例1：标量和张量\n",
    "a = torch.tensor([1, 2, 3])           # shape: [3]\n",
    "b = 2                                 # shape: []\n",
    "c = a * b                            # shape: [3]\n",
    "print(c)\n",
    "# b被广播到[3]：[2, 2, 2]\n",
    "\n",
    "# 示例2：不同维度张量\n",
    "x = torch.tensor([[1], [2], [3]])    # shape: [3, 1]\n",
    "y = torch.tensor([4, 5, 6])          # shape: [3]\n",
    "z = x + y                            # shape: [3, 3]\n",
    "# x被广播到[3, 3]：[[1, 1, 1], [2, 2, 2], [3, 3, 3]]\n",
    "print(z)\n",
    "\n",
    "# 矩阵运算中的广播\n",
    "batch1 = torch.randn(1, 3, 4)        # shape: [1, 3, 4]\n",
    "batch2 = torch.randn(2, 3, 4)        # shape: [2, 3, 4]\n",
    "result = batch1 + batch2             # shape: [2, 3, 4]\n",
    "# batch1的第一维从1广播到2\n",
    "# batch1被广播到[2, 3, 4]的shape\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. torch.matmul 或 @ 通用矩阵乘法运算符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5]),\n",
       " tensor([[[ 1.3635, -0.9821,  0.4979,  0.0790, -0.8105],\n",
       "          [-0.4579,  1.3398, -0.2127,  0.2078,  0.3870],\n",
       "          [ 1.0137,  2.6641,  3.4173, -0.7610,  1.2600]],\n",
       " \n",
       "         [[-1.4139,  1.5463, -4.4714,  1.8290, -0.0223],\n",
       "          [ 2.9376,  3.0884,  0.7397,  0.9518,  1.0776],\n",
       "          [ 2.6754,  0.1563,  1.9222,  0.1724, -1.1409]]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(2, 3, 4)\n",
    "B = torch.randn(1, 4, 5)\n",
    "C = torch.matmul(A, B)  # 或 C = A @ B\n",
    "C.shape, C\n",
    "# 用途：通用矩阵乘法，可以处理多种情况\n",
    "# 特点：\n",
    "# 支持广播机制（broadcasting）\n",
    "# 可以处理不同维度的张量\n",
    "# 根据输入维度自动选择合适的乘法方式\n",
    "# 适用场景：\n",
    "# 当不确定具体需要哪种矩阵乘法时\n",
    "# 需要利用广播机制时"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. torch.mm 标准2d矩阵的乘法运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4]),\n",
       " tensor([[ 1.1599,  0.9311, -0.0379, -0.5269],\n",
       "         [-2.0701,  0.1375, -0.1265,  0.2984]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(2, 3)\n",
    "B = torch.randn(3, 4)\n",
    "C = torch.mm(A, B)\n",
    "C.shape, C\n",
    "# 用途：二维矩阵乘法\n",
    "# 特点：\n",
    "# 只能处理2D张量\n",
    "# 比matmul运行更快\n",
    "# 不支持广播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. torch.bmm 专用于3d的矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5]),\n",
       " tensor([[[-3.3989, -2.1715, -0.4937, -6.3157,  2.3156],\n",
       "          [ 1.5177,  0.5540,  1.0551,  1.4014,  0.5849],\n",
       "          [-0.1485, -0.0367, -0.2739, -0.9223, -0.3220]],\n",
       " \n",
       "         [[-0.6912, -1.5945,  0.8291,  3.4820,  4.4763],\n",
       "          [ 1.2197,  4.0576,  0.2921, -3.8629, -0.6839],\n",
       "          [ 0.9048,  0.4832,  1.7965, -0.4360, -6.3062]]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(2, 3, 4)\n",
    "B = torch.randn(2, 4, 5)\n",
    "C = torch.bmm(A, B)\n",
    "C.shape, C\n",
    "# 用途：批量矩阵乘法\n",
    "# 特点：\n",
    "# 只能处理3D张量\n",
    "# 比matmul运行更快\n",
    "# 不支持广播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. torch.mv 适用于 2d与1d的相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), tensor([-1.1414, -1.0777]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(2, 3)\n",
    "B = torch.randn(3)\n",
    "C = torch.mv(A, B)\n",
    "C.shape, C\n",
    "# 用途：矩阵乘以向量\n",
    "# 特点：\n",
    "# 只能处理2D矩阵和1D向量\n",
    "# 适用场景：\n",
    "# 线性变换\n",
    "# 神经网络中的全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量堆叠和连接方法\n",
    "\n",
    "1. **内存效率**：\n",
    "   - `cat`和`stack`会创建新的内存\n",
    "   - `view`和`expand`通常更内存效率\n",
    "\n",
    "2. **速度**：\n",
    "   - `vstack`/`hstack`比直接使用`cat`稍慢\n",
    "   - `expand`比`repeat`更快但功能有限\n",
    "\n",
    "3. **建议**：\n",
    "   - 对于大量小张量，优先使用`stack`\n",
    "   - 需要节省内存时，考虑使用`expand`\n",
    "   - 处理动态形状时，`cat`更灵活\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.维度操作(view reshape)\n",
    "\n",
    "主要区别：\n",
    "1. 内存连续性：\n",
    "   - `view`要求张量必须是连续的\n",
    "   - `reshape`不要求连续，更灵活\n",
    "   - `contiguous`用于确保连续性\n",
    "\n",
    "2. 内存共享：\n",
    "   - `view`总是共享内存\n",
    "   - `reshape`尽可能共享内存，但不保证\n",
    "   - `contiguous`在需要时会创建新的内存\n",
    "\n",
    "3. 使用建议：\n",
    "   - 如果确定张量是连续的，用`view`更高效\n",
    "   - 如果不确定是否连续，用`reshape`更安全\n",
    "   - 如果需要确保连续性，用`contiguous`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6305,  0.3605, -0.0297, -0.6499, -0.1706, -0.2751,  0.3029,  0.4524,\n",
      "        -0.9376, -0.0430,  0.4875, -1.4365, -0.0883,  0.7324,  0.8828, -2.3603]) torch.Size([16])\n",
      "tensor([[ 0.6305,  0.3605, -0.0297, -0.6499, -0.1706, -0.2751,  0.3029,  0.4524],\n",
      "        [-0.9376, -0.0430,  0.4875, -1.4365, -0.0883,  0.7324,  0.8828, -2.3603]]) torch.Size([2, 8])\n",
      "tensor([[-0.1914, -0.8185,  0.8679,  2.1392],\n",
      "        [-0.9743, -0.5182, -0.5222,  0.9768],\n",
      "        [-1.7901,  1.8443,  0.3530, -1.6218],\n",
      "        [ 0.0425, -0.2054, -2.1093, -1.8665]]) torch.Size([4, 4])\n",
      "tensor([[-0.1914, -0.8185,  0.8679,  2.1392, -0.9743, -0.5182, -0.5222,  0.9768],\n",
      "        [-1.7901,  1.8443,  0.3530, -1.6218,  0.0425, -0.2054, -2.1093, -1.8665]]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# 1. `view`操作：\n",
    "# - 作用：改变张量的形状而不改变其数据\n",
    "# - 特点：\n",
    "#   - 要求张量在内存中是连续的（contiguous）\n",
    "#   - 与原始张量共享内存\n",
    "#   - 如果原始张量不是连续的，`view`会失败\n",
    "# - 示例：\n",
    "\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)  # 将4x4变成16\n",
    "print(y, y.shape)\n",
    "y = x.view(-1, 8)  # -1表示自动计算该维度大小\n",
    "print(y, y.shape)\n",
    "\n",
    "\n",
    "# 2. `reshape`操作：\n",
    "# - 作用：改变张量的形状\n",
    "# - 特点：\n",
    "#   - 更加灵活，不要求张量在内存中连续\n",
    "#   - 如果可能，会尝试返回一个共享内存的视图\n",
    "#   - 如果不可能共享内存，会返回一个新的张量\n",
    "# - 示例：\n",
    "\n",
    "x = torch.randn(4, 4)\n",
    "print(x, x.shape)\n",
    "y = x.reshape(2, 8)  # 总是能工作，即使x不是连续的\n",
    "print(y, y.shape)\n",
    "\n",
    "# 3. `contiguous`操作：\n",
    "# - 作用：确保张量在内存中是连续的\n",
    "# - 特点：\n",
    "#   - 如果张量已经是连续的，返回自身\n",
    "#   - 如果不是连续的，返回一个新的连续张量\n",
    "# - 示例：\n",
    "\n",
    "x = torch.randn(4, 4)\n",
    "x = x.transpose(0, 1)  # transpose后可能不连续\n",
    "x = x.contiguous()  # 确保连续\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 多头注意力中的维度变换\n",
    "x = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "# 方法1：使用view（如果x是连续的）\n",
    "x = x.view(batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "# 方法2：使用reshape（更安全）\n",
    "x = x.reshape(batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "# 方法3：确保连续后使用view \n",
    "x = x.contiguous().view(batch_size, seq_len, num_heads, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4])\n",
      "torch.Size([3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "### unsqueeze (增加维度)\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "x_new = x.unsqueeze(0)  # shape: [1, 3, 4]\n",
    "print(x_new.shape)\n",
    "# 或\n",
    "x_new = x.unsqueeze(-1)  # shape: [3, 4, 1]\n",
    "print(x_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "### squeeze (移除维度) squeeze()在默认情况下会删除所有维度大小为1的维度。让我详细解释一下：\n",
    "x = torch.randn(1, 3, 1, 4, 1)\n",
    "x_new = x.squeeze(0)    # 只删除第0维\n",
    "print(x_new.shape)      # torch.Size([3, 1, 4, 1])\n",
    "\n",
    "x_new = x.squeeze(2)    # 只删除第2维\n",
    "print(x_new.shape)      # torch.Size([1, 3, 4, 1])\n",
    "\n",
    "# 如果指定维度的大小不为1，则不会发生任何变化\n",
    "x_new = x.squeeze(1)    # 第1维大小为3，不会被删除\n",
    "print(x_new.shape)      # torch.Size([1, 3, 1, 4, 1])\n",
    "\n",
    "x.squeeze_()  # 直接修改x，不返回新张量 实际上，似乎所有后面带上_的函数都是直接修改张量，不返回新张量\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 堆叠操作 (Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4])\n",
      "torch.Size([3, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "### torch.stack\n",
    "# 在新维度上堆叠\n",
    "tensors = [torch.randn(3, 4) for _ in range(5)]\n",
    "stacked = torch.stack(tensors, dim=0)  # shape: [5, 3, 4]\n",
    "print(stacked.shape)\n",
    "stacked = torch.stack(tensors, dim=1)  # shape: [3, 5, 4]\n",
    "print(stacked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 连接操作 (Concatenation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### torch.cat\n",
    "\n",
    "# 在现有维度上连接\n",
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(3, 4)\n",
    "c = torch.cat([a, b], dim=0)  # shape: [6, 4]\n",
    "c = torch.cat([a, b], dim=1)  # shape: [3, 8]\n",
    "c.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. *stack操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### torch.vstack (垂直堆叠)\n",
    "\n",
    "# 等价于 torch.cat([a, b], dim=0)\n",
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(3, 4)\n",
    "c = torch.vstack([a, b])  # shape: [6, 4]\n",
    "\n",
    "\n",
    "### torch.hstack (水平堆叠)\n",
    "\n",
    "# 等价于 torch.cat([a, b], dim=1)\n",
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(3, 4)\n",
    "c = torch.hstack([a, b])  # shape: [3, 8]\n",
    "\n",
    "\n",
    "### torch.dstack (深度堆叠)\n",
    "\n",
    "# 在深度方向堆叠\n",
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(3, 4)\n",
    "c = torch.dstack([a, b])  # shape: [3, 4, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 复制和重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]])\n",
      "tensor([[9, 2, 3, 1, 2, 3, 1, 2, 3],\n",
      "        [1, 2, 3, 1, 2, 3, 1, 2, 3]])\n",
      "tensor([[9, 2, 3]])\n",
      "tensor([[9, 2, 3],\n",
      "        [9, 2, 3],\n",
      "        [9, 2, 3],\n",
      "        [9, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "### repeat\n",
    "\n",
    "x = torch.tensor([[1, 2, 3]])  # shape: [1, 3]\n",
    "# 在各个维度上重复\n",
    "y = x.repeat(2, 3)  # shape: [4, 9]\n",
    "y[0, 0] = 9  # 修改y不会影响x，因为是独立的副本\n",
    "print(x)  # 仍然是[[1, 2, 3]]\n",
    "print(y)\n",
    "\n",
    "### expand\n",
    "\n",
    "x = torch.tensor([[1, 2, 3]])\n",
    "# 广播式扩展，不复制数据\n",
    "y = x.expand(4, 3)  # shape: [4, 3]\n",
    "y[0, 0] = 9  # 修改y会影响x，因为共享内存\n",
    "print(x)  # 也会变成\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 实战中的常用组合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 批处理数据准备\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单样本转批量\n",
    "single_sample = torch.randn(3, 4)\n",
    "batch = single_sample.unsqueeze(0)  # 添加批次维度 [1, 3, 4]\n",
    "\n",
    "# 多个样本组批\n",
    "samples = [torch.randn(3, 4) for _ in range(5)]\n",
    "batch = torch.stack(samples, dim=0)  # [5, 3, 4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. 特征拼接\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 30]), torch.Size([32, 30]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在特征维度上拼接\n",
    "feature1 = torch.randn(32, 10)\n",
    "feature2 = torch.randn(32, 20)\n",
    "combined = torch.cat([feature1, feature2], dim=1)  # [32, 30]\n",
    "combined_hstack = torch.hstack([feature1, feature2])  # [32, 30]\n",
    "combined.shape, combined_hstack.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. 序列处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理变长序列\n",
    "sequences = [torch.randn(length, 128) for length in [5, 3, 4]]\n",
    "padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. 图像处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 添加通道维度\n",
    "image = torch.randn(28, 28)\n",
    "with_channel = image.unsqueeze(0)  # [1, 28, 28]\n",
    "\n",
    "# 批量处理\n",
    "images = [torch.randn(1, 28, 28) for _ in range(4)]\n",
    "batch = torch.cat(images, dim=0)  # [4, 1, 28, 28]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([32, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### 5. 注意力机制\n",
    "\n",
    "# 多头注意力的头拆分\n",
    "batch_size, n_heads = 32, 8\n",
    "hidden_dim = 512\n",
    "x = torch.randn(batch_size, hidden_dim)\n",
    "print(x.shape)\n",
    "# 重塑为多头形式\n",
    "reshaped = x.view(batch_size, n_heads, hidden_dim // n_heads)\n",
    "print(reshaped.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
